# Healthcare Data Lakehouse using GCP

A comprehensive end-to-end data engineering project that unifies and processes multi-source healthcare datasets (EHR, claims, IoT device data) in a HIPAA-compliant architecture using GCP-native tools.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Real-time     â”‚    â”‚   Batch Sources â”‚
â”‚                 â”‚    â”‚   IoT Devices   â”‚    â”‚                 â”‚
â”‚ â€¢ EHR Systems   â”‚    â”‚ â€¢ Heart Rate    â”‚    â”‚ â€¢ Claims Data   â”‚
â”‚ â€¢ Claims Data   â”‚    â”‚ â€¢ Temperature   â”‚    â”‚ â€¢ Patient Info  â”‚
â”‚ â€¢ IoT Devices   â”‚    â”‚ â€¢ Blood Pressureâ”‚    â”‚ â€¢ Lab Results   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Cloud Pub/Sub         â”‚
                    â”‚   (Message Queue)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Cloud Dataflow        â”‚
                    â”‚   (Streaming/Batch ETL)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Google Cloud Storage   â”‚
                    â”‚   (Data Lake)            â”‚
                    â”‚                          â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                    â”‚ â”‚  Raw    â”‚  Processed â”‚â”‚
                    â”‚ â”‚  Zone   â”‚   Zone     â”‚â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      BigQuery            â”‚
                    â”‚   (Data Warehouse)       â”‚
                    â”‚                          â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                    â”‚ â”‚   Curated Zone        â”‚â”‚
                    â”‚ â”‚ â€¢ Fact Tables         â”‚â”‚
                    â”‚ â”‚ â€¢ Dimension Tables    â”‚â”‚
                    â”‚ â”‚ â€¢ Analytics Views     â”‚â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Looker              â”‚
                    â”‚   (BI & Analytics)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Google Cloud Platform account with billing enabled
- Terraform installed (v1.0+)
- Python 3.8+
- Google Cloud SDK
- Docker (for local development)

### Setup Instructions

1. **Clone and Navigate**
   ```bash
   cd "Healthcare Data Lakehouse using GCP"
   ```

2. **Initialize GCP Project**
   ```bash
   # Set your GCP project ID
   export PROJECT_ID="your-project-id"
   gcloud config set project $PROJECT_ID
   
   # Enable required APIs
   gcloud services enable \
     compute.googleapis.com \
     storage.googleapis.com \
     bigquery.googleapis.com \
     pubsub.googleapis.com \
     dataflow.googleapis.com \
     composer.googleapis.com \
     cloudkms.googleapis.com \
     logging.googleapis.com \
     monitoring.googleapis.com
   ```

3. **Deploy Infrastructure**
   ```bash
   cd terraform
   terraform init
   terraform plan -var="project_id=$PROJECT_ID"
   terraform apply -var="project_id=$PROJECT_ID"
   ```

4. **Deploy Data Pipeline**
   ```bash
   cd ../dataflow
   python deploy_pipelines.py
   ```

5. **Run dbt Models**
   ```bash
   cd ../dbt
   dbt deps
   dbt run
   ```

6. **Start Data Ingestion**
   ```bash
   cd ../ingestion
   python start_ingestion.py
   ```

## ğŸ“ Project Structure

```
Healthcare Data Lakehouse using GCP/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ terraform/                          # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                        # Main Terraform configuration
â”‚   â”œâ”€â”€ variables.tf                   # Variable definitions
â”‚   â”œâ”€â”€ outputs.tf                     # Output values
â”‚   â””â”€â”€ modules/                       # Reusable Terraform modules
â”œâ”€â”€ ingestion/                         # Data ingestion scripts
â”‚   â”œâ”€â”€ publishers/                    # Pub/Sub publishers
â”‚   â”œâ”€â”€ subscribers/                   # Pub/Sub subscribers
â”‚   â””â”€â”€ data_generators/               # Synthetic data generators
â”œâ”€â”€ dataflow/                          # Apache Beam pipelines
â”‚   â”œâ”€â”€ pipelines/                     # Dataflow job definitions
â”‚   â”œâ”€â”€ templates/                     # Job templates
â”‚   â””â”€â”€ utils/                         # Pipeline utilities
â”œâ”€â”€ dbt/                               # Data transformation models
â”‚   â”œâ”€â”€ models/                        # dbt models
â”‚   â”œâ”€â”€ macros/                        # Reusable macros
â”‚   â””â”€â”€ dbt_project.yml                # dbt configuration
â”œâ”€â”€ airflow/                           # Orchestration DAGs
â”‚   â”œâ”€â”€ dags/                          # Airflow DAG definitions
â”‚   â””â”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ security/                          # Security configurations
â”‚   â”œâ”€â”€ iam/                           # IAM policies and roles
â”‚   â””â”€â”€ compliance/                    # HIPAA compliance checks
â”œâ”€â”€ monitoring/                        # Monitoring and alerting
â”‚   â”œâ”€â”€ dashboards/                    # Cloud Monitoring dashboards
â”‚   â””â”€â”€ alerts/                        # Alert policies
â””â”€â”€ docs/                              # Documentation
    â”œâ”€â”€ architecture/                  # Architecture diagrams
    â””â”€â”€ api/                           # API documentation
```

## ğŸ”’ Security & Compliance

This project implements HIPAA-compliant security measures:

- **Encryption**: All data encrypted at rest and in transit
- **Access Control**: IAM with least privilege principles
- **Audit Logging**: Comprehensive logging for compliance
- **Network Security**: VPC with private service access
- **Data Governance**: Data lineage and cataloging

## ğŸ“Š Data Flow

1. **Ingestion**: Real-time IoT data and batch claims data ingested via Pub/Sub
2. **Processing**: Dataflow pipelines clean and transform data
3. **Storage**: Raw data stored in GCS, processed data in BigQuery
4. **Analytics**: dbt models create analytics-ready tables
5. **Visualization**: Looker dashboards provide insights

## ğŸ› ï¸ Technologies Used

- **Infrastructure**: Terraform, Google Cloud Platform
- **Storage**: Google Cloud Storage, BigQuery
- **Processing**: Apache Beam (Dataflow)
- **Orchestration**: Apache Airflow (Cloud Composer)
- **Transformation**: dbt (data build tool)
- **Visualization**: Looker
- **Security**: Cloud KMS, IAM, VPC Service Controls

## ğŸ“ˆ Monitoring & Alerting

- Real-time pipeline monitoring via Cloud Monitoring
- Custom dashboards for operational metrics
- Alert policies for data quality and pipeline health
- Audit logs for compliance reporting

## ğŸš€ Demo Scenarios

1. **Real-time Patient Monitoring**: Stream IoT vitals data
2. **Claims Processing**: Batch process insurance claims
3. **Operational Analytics**: Hospital administration KPIs
4. **Compliance Reporting**: HIPAA audit trail generation

## ğŸ“ License

This project is for educational and portfolio purposes.

## ğŸ¤ Contributing

This is a demonstration project. For production use, ensure proper security reviews and compliance validation. 